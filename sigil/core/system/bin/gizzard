#!/usr/bin/env python3
import re
import os
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import yaml
import jsonschema
from dataclasses import dataclass
from collections import defaultdict
import argparse
import glob
import json
from jsonschema import validate, ValidationError

@dataclass
class TokenStats:
    original_tokens: int
    processed_tokens: int
    reduction_ratio: float
    file_path: str

class GizzardProcessor:
    def __init__(self, config_path: str, schema_path: str):
        # Load and validate configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)['gizzard_processing']
        
        with open(schema_path, 'r') as f:
            self.schema = yaml.safe_load(f)['gizzard_processing']
        
        try:
            jsonschema.validate(instance=self.config, schema=self.schema)
        except jsonschema.exceptions.ValidationError as e:
            print(f"Configuration validation error: {e}")
            raise
        
        # Initialize from config
        self.categories = self.config['symbol_mapping']['categories']
        self.patterns = {
            r'requires|needs|must have': self.config['relationship_notation']['arrow'],
            r'and|while|but': self.config['relationship_notation']['relationship_separator'],
            r'or': self.config['relationship_notation']['alternative_separator'],
            r'not|never|without': self.config['content_reduction']['negation_prefix'],
            r'the|a|an': '',  # Remove articles
            r'is|are|was|were': '',  # Remove auxiliary verbs
        }
        
        # Special terms that should not be reduced
        self.preserve_terms = set()
        
        # Token statistics
        self.token_stats: Dict[str, TokenStats] = {}
        
        # Get WONDER_ROOT
        self.wonder_root = os.environ.get('WONDER_ROOT')
        if not self.wonder_root:
            raise ValueError("WONDER_ROOT environment variable must be set")
        self.wonder_root = Path(self.wonder_root)
        
        self.all_content = []
        self.all_relationships = []
        
        # Define kernel schema
        self.kernel_schema = {
            "type": "object",
            "required": ["kernel", "metadata", "identity", "actions", "content", "relationships"],
            "properties": {
                "kernel": {
                    "type": "object",
                    "required": ["name", "repository", "seed_file"],
                    "properties": {
                        "name": {"type": "string"},
                        "repository": {"type": "string"},
                        "seed_file": {"type": "string"}
                    }
                },
                "metadata": {
                    "type": "object",
                    "required": ["version", "description"],
                    "properties": {
                        "version": {"type": "string"},
                        "description": {"type": "string"}
                    }
                },
                "identity": {
                    "type": "object",
                    "required": ["name", "description"],
                    "properties": {
                        "name": {"type": "string"},
                        "description": {"type": "string"}
                    }
                },
                "actions": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "content": {
                    "type": "array",
                    "items": {
                        "type": "object",
                        "required": ["title", "content", "relationships"],
                        "properties": {
                            "title": {"type": "string"},
                            "content": {"type": "string"},
                            "relationships": {
                                "type": "array",
                                "items": {"type": "string"}
                            }
                        }
                    }
                },
                "relationships": {
                    "type": "array",
                    "items": {"type": "string"}
                }
            }
        }
        
        self.sigil_dirs = [
            os.path.join(self.wonder_root, "sigil/ethic"),
            os.path.join(self.wonder_root, "sigil/concept"),
            os.path.join(self.wonder_root, "sigil/metareal"),
            os.path.join(self.wonder_root, "sigil/orthoreal"),
            os.path.join(self.wonder_root, "sigil/parareal"),
            os.path.join(self.wonder_root, "sigil/hyperreal"),
            os.path.join(self.wonder_root, "sigil/hyporeal")
        ]
        self.sigil_files = []
        self.load_sigil_files()
        
    def resolve_path(self, path: str) -> Path:
        """Resolve a path relative to WONDER_ROOT."""
        if path.startswith('/'):
            return Path(path)
        return self.wonder_root / path
    
    def estimate_tokens(self, text: str) -> int:
        """Estimate the number of tokens in a text string."""
        # Simple estimation: split on whitespace and count words
        # This is a rough approximation - in reality, tokenization is more complex
        return len(text.split())
    
    def load_preserve_terms(self, filepath: str) -> None:
        """Load terms that should not be reduced from a YAML file."""
        filepath = self.resolve_path(filepath)
        with open(filepath, 'r') as f:
            self.preserve_terms = set(yaml.safe_load(f)['preserve_terms'])
    
    def identify_category(self, content: str) -> Optional[str]:
        """Identify the category of the content based on file path or content."""
        for category, prefix in self.categories.items():
            if category in content.lower():
                return prefix
        return None
    
    def reduce_content(self, text: str) -> str:
        """Apply content reduction rules to the text."""
        if self.config['content_reduction']['remove_articles']:
            text = re.sub(r'the|a|an', '', text, flags=re.IGNORECASE)
        
        if self.config['content_reduction']['remove_auxiliary_verbs']:
            text = re.sub(r'is|are|was|were', '', text, flags=re.IGNORECASE)
        
        # Split into sentences and process each
        sentences = re.split(r'[.!?]+', text)
        reduced = []
        
        for sentence in sentences:
            # Skip empty sentences
            if not sentence.strip():
                continue
                
            # Split into words and process
            words = sentence.strip().split()
            processed = []
            
            for word in words:
                # Preserve special terms
                if word in self.preserve_terms:
                    processed.append(word)
                else:
                    # Remove common suffixes
                    word = re.sub(r'ing$|ed$|s$', '', word)
                    processed.append(word)
            
            reduced.append(' '.join(processed))
        
        return self.config['relationship_notation']['relationship_separator'].join(reduced)
    
    def extract_relationships(self, text: str) -> List[Tuple[str, str]]:
        """Extract relationships from the text."""
        relationships = []
        
        # Look for relationship indicators
        rel_patterns = [
            r'emphasizes|focuses on|centers around',
            r'requires|needs|must have',
            r'relates to|connects with|links to'
        ]
        
        for pattern in rel_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                # Extract the related concepts
                before = text[:match.start()].strip()
                after = text[match.end():].strip()
                if before and after:
                    relationships.append((before, after))
        
        return relationships
    
    def process_file(self, input_path: str) -> Optional[Tuple[str, List[Tuple[str, str]]]]:
        """Process a markdown file and return its processed content and relationships."""
        input_path = self.resolve_path(input_path)
        print(f"Processing file: {input_path}")  # Debug
        
        with open(input_path, 'r') as f:
            content = f.read()
        
        # Extract title and category
        title_match = re.match(r'#\s+(.+?)(?:\n|$)', content)
        if not title_match:
            print(f"No title found in {input_path}")  # Debug
            return None
        
        title = title_match.group(1)
        category = self.identify_category(str(input_path))
        if not category:
            print(f"No category found for {input_path}")  # Debug
            return None
        
        print(f"Found title: {title}, category: {category}")  # Debug
        
        # Extract relationships
        relationships = self.extract_relationships(content)
        print(f"Found {len(relationships)} relationships")  # Debug
        
        # Process the content
        processed = []
        processed.append(f"{category}:{title.lower().replace(' ', '-')}")
        
        # Process the main content (everything after the title)
        main_content = content[title_match.end():].strip()
        if main_content:
            reduced_content = self.reduce_content(main_content)
            if reduced_content:
                processed.append(reduced_content)
        
        # Add relationship statements
        for before, after in relationships:
            reduced_before = self.reduce_content(before)
            reduced_after = self.reduce_content(after)
            if reduced_before and reduced_after:
                processed.append(f"{self.config['relationship_notation']['arrow']}{reduced_before}{self.config['relationship_notation']['relationship_separator']}{reduced_after}")
        
        # Calculate token statistics
        original_tokens = self.estimate_tokens(content)
        processed_content = '\n'.join(processed)
        processed_tokens = self.estimate_tokens(processed_content)
        reduction_ratio = (original_tokens - processed_tokens) / original_tokens
        
        print(f"Processed content: {processed_content}")  # Debug
        
        # Store statistics
        self.token_stats[str(input_path)] = TokenStats(
            original_tokens=original_tokens,
            processed_tokens=processed_tokens,
            reduction_ratio=reduction_ratio,
            file_path=str(input_path)
        )
        
        self.all_content.append(processed_content)
        self.all_relationships.extend(relationships)
        
        return processed_content, relationships
    
    def validate_wonder_root(self):
        """Validate that WONDER_ROOT is set and points to a valid directory.
        Returns (is_valid, error_message)"""
        if not self.wonder_root:
            return False, "WONDER_ROOT environment variable must be set"
        
        if not os.path.isdir(self.wonder_root):
            return False, f"WONDER_ROOT directory does not exist: {self.wonder_root}"
        
        return True, None

    def clean_content(self, content: str) -> str:
        """Clean and reduce content while preserving special terms and intelligently handling long sentences."""
        # Define special terms to preserve
        special_terms = {
            "metareal", "orthoreal", "parareal", "hyperreal", "hyporeal",
            "Rokolisk", "Wonder", "Cinder", "sigil", "kernel", "ethic",
            "∧", "∨", "¬", "→", "↔", "∀", "∃", "∈", "∉", "⊂", "⊃", "∪", "∩"
        }
        
        # Split into sentences
        sentences = re.split(r'[.!?]+', content)
        cleaned_sentences = []
        
        for sentence in sentences:
            # Skip empty sentences
            if not sentence.strip():
                continue
                
            # Clean up whitespace
            sentence = sentence.strip()
            words = sentence.split()
            
            # Skip if sentence is too short
            if len(words) < 3:
                continue
                
            # Check if sentence contains special terms
            special_terms_found = [term for term in special_terms if term.lower() in sentence.lower()]
            
            # Process words
            cleaned_words = []
            skip_next = False
            
            for i, word in enumerate(words):
                if skip_next:
                    skip_next = False
                    continue
                    
                # Preserve special terms
                if any(term.lower() in word.lower() for term in special_terms_found):
                    cleaned_words.append(word)
                    continue
                    
                # Skip common words and patterns
                if word.lower() in {
                    'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',
                    'is', 'are', 'was', 'were', 'be', 'been', 'being',
                    'have', 'has', 'had', 'having',
                    'do', 'does', 'did', 'doing',
                    'can', 'could', 'will', 'would', 'shall', 'should',
                    'may', 'might', 'must',
                    'very', 'really', 'quite', 'rather', 'somewhat',
                    'just', 'only', 'also', 'then', 'than',
                    'that', 'which', 'who', 'whom', 'whose',
                    'when', 'where', 'why', 'how',
                    'this', 'these', 'those', 'there',
                    'about', 'into', 'onto', 'upon',
                    'among', 'between', 'through'
                }:
                    continue
                    
                # Skip redundant pairs
                if word.lower() in {'completely', 'totally', 'absolutely', 'entirely', 'fully', 'thoroughly'}:
                    skip_next = True
                    continue
                    
                # Remove common suffixes
                word = re.sub(r'(ing|ed|ly|ment|ness|tion|sion)$', '', word)
                
                # Only add if word is not empty after processing
                if word and len(word) > 1:
                    cleaned_words.append(word)
            
            # Handle long sentences through summarization
            if len(cleaned_words) > 30:
                # Extract key components: subject, verb, object, and special terms
                key_terms = []
                for word in cleaned_words:
                    if any(term.lower() in word.lower() for term in special_terms_found):
                        key_terms.append(word)
                
                # Keep the first and last few words to maintain context
                context_start = ' '.join(cleaned_words[:3])
                context_end = ' '.join(cleaned_words[-3:])
                
                # Combine key components with context
                if key_terms:
                    # If we have special terms, use them as anchors
                    compressed = f"{context_start} {' '.join(key_terms)} {context_end}"
                else:
                    # If no special terms, keep the most important parts
                    compressed = f"{context_start} ... {context_end}"
                
                cleaned_sentences.append(compressed)
            else:
                # For shorter sentences, just join the cleaned words
                cleaned_sentences.append(' '.join(cleaned_words))
        
        # Join sentences with proper spacing, removing any that became too short
        return ' '.join(s for s in cleaned_sentences if len(s.split()) >= 3)

    def validate_kernel(self, kernel: Dict[str, Any]) -> bool:
        """Validate kernel structure against schema."""
        try:
            validate(instance=kernel, schema=self.kernel_schema)
            return True
        except ValidationError as e:
            print(f"Validation error: {str(e)}")
            return False

    def process_kernel(self, kernel_path: str, output_path: str) -> bool:
        """Process a kernel configuration file."""
        # First validate WONDER_ROOT
        is_valid, error = self.validate_wonder_root()
        if not is_valid:
            print(error)
            return False
        
        print(f"Loading kernel from: {os.path.abspath(kernel_path)}")
        
        # Validate YAML syntax
        is_valid, error = validate_yaml_file(kernel_path)
        if not is_valid:
            print(error)
            return False
        
        # Load and validate kernel schema
        with open(kernel_path, 'r') as f:
            try:
                kernel = yaml.safe_load(f)
                kernel_name = next(iter(kernel))
                kernel_data = kernel[kernel_name]
                
                # Update sigil_dirs based on kernel configuration
                self.sigil_dirs = [os.path.join(self.wonder_root, sigil) for sigil in kernel_data.get('sigils', [])]
                self.sigil_files = []
                self.load_sigil_files()
                
                print(f"Found {len(self.sigil_files)} files to process")
                
                # Process the kernel data
                processed = self.process_kernel_data(kernel_data)
                
                # Write output
                print(f"Writing output to: {output_path}")
                self.write_output(kernel_name, processed, output_path)
                
                # Print token reduction statistics
                if self.token_stats:
                    self.print_token_stats()
                else:
                    print("No files were processed. Please check the paths in your kernel configuration.")
                
                return True
                
            except Exception as e:
                print(f"Error loading kernel configuration: {str(e)}")
                return False

    def process_kernel_data(self, kernel_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process the kernel content."""
        # Transform the kernel data to match our internal structure
        processed = {
            "kernel": {
                "name": "cinder_picokernel",
                "repository": kernel_data.get("repo", ""),
                "seed_file": kernel_data.get("seed", "")
            },
            "metadata": {
                "version": "1.0",
                "description": kernel_data.get("prompt", "")
            },
            "identity": {
                "name": "Cinder",
                "description": kernel_data.get("identity", "")
            },
            "actions": [action.strip() for action in kernel_data.get("actions", "").split(".")],
            "content": [],
            "relationships": []
        }
        
        # Track seen relationships to prevent duplicates
        seen_relationships = set()
        
        # Process each sigil file
        for sigil_file in self.sigil_files:
            try:
                print(f"Processing file: {sigil_file}")  # Debug
                
                # Read and process the markdown file
                with open(sigil_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Extract title and content
                title_match = re.match(r'^#\s+(.+)$', content, re.MULTILINE)
                title = title_match.group(1) if title_match else os.path.splitext(os.path.basename(sigil_file))[0]
                
                # Extract relationships
                relationships = []
                for rel_match in re.finditer(r'\[\[(.*?)\]\]', content):
                    rel = rel_match.group(1)
                    if rel not in seen_relationships:
                        relationships.append(rel)
                        seen_relationships.add(rel)
                
                # Clean up content
                content = re.sub(r'\[\[(.*?)\]\]', r'\1', content)  # Remove link syntax
                content = re.sub(r'^#\s+.*$', '', content, flags=re.MULTILINE)  # Remove headers
                content = re.sub(r'\n{3,}', '\n\n', content)  # Normalize spacing
                content = content.strip()
                
                # Apply content reduction
                original_tokens = len(content.split())
                content = self.clean_content(content)
                processed_tokens = len(content.split())
                
                # Add to processed content
                processed["content"].append({
                    "title": title,
                    "content": content,
                    "relationships": relationships
                })
                
                # Add relationships to global list if not already present
                for rel in relationships:
                    if rel not in seen_relationships:
                        processed["relationships"].append(rel)
                        seen_relationships.add(rel)
                
                # Log token reduction
                reduction = ((original_tokens - processed_tokens) / original_tokens) * 100 if original_tokens > 0 else 0
                print(f"Token reduction for {os.path.basename(sigil_file)}: {reduction:.2f}%")
                
                # Store statistics
                self.token_stats[str(sigil_file)] = TokenStats(
                    original_tokens=original_tokens,
                    processed_tokens=processed_tokens,
                    reduction_ratio=reduction / 100,
                    file_path=str(sigil_file)
                )
                
            except Exception as e:
                print(f"Error processing {sigil_file}: {str(e)}")
                continue
        
        return processed

    def write_output(self, kernel_name, kernel_data, output_path):
        """Write the processed content to the output file."""
        # Create the optimized content for ChatGPT ingestion
        optimized_content = {
            'kernel': kernel_name,
            'metadata': {
                'repository': kernel_data['kernel']['repository'],
                'seed': kernel_data['kernel']['seed_file'],
                'version': kernel_data['metadata']['version'],
                'description': kernel_data['metadata']['description']
            },
            'identity': kernel_data['identity'],
            'actions': [action for action in kernel_data['actions'] if action],  # Filter out empty actions
            'content': kernel_data['content'],
            'relationships': kernel_data['relationships']
        }
        
        # Write the optimized output
        with open(output_path, 'w') as f:
            yaml.dump(optimized_content, f, sort_keys=False, default_flow_style=False, allow_unicode=True)

    def print_token_stats(self):
        """Print token reduction statistics to stdout."""
        print("\nToken Reduction Statistics:")
        print("-" * 80)
        print(f"{'File':<50} {'Original':>10} {'Processed':>10} {'Reduction':>10}")
        print("-" * 80)
        
        total_original = 0
        total_processed = 0
        
        for stats in self.token_stats.values():
            print(f"{stats.file_path:<50} {stats.original_tokens:>10} {stats.processed_tokens:>10} {stats.reduction_ratio:>10.2%}")
            total_original += stats.original_tokens
            total_processed += stats.processed_tokens
        
        total_reduction = (total_original - total_processed) / total_original
        print("-" * 80)
        print(f"{'TOTAL':<50} {total_original:>10} {total_processed:>10} {total_reduction:>10.2%}")

    def load_sigil_files(self):
        """Load all sigil files from the specified directories."""
        for sigil_dir in self.sigil_dirs:
            for root, _, files in os.walk(sigil_dir):
                for file in files:
                    if file.endswith(('.md', '.markdown')):
                        self.sigil_files.append(os.path.join(root, file))

def validate_yaml_file(file_path):
    """Validate a YAML file for syntax and structure.
    Returns (is_valid, error_message)"""
    try:
        with open(file_path, 'r') as f:
            yaml.safe_load(f)
        return True, None
    except yaml.YAMLError as e:
        return False, f"YAML validation error in {file_path}: {str(e)}"
    except Exception as e:
        return False, f"Error reading {file_path}: {str(e)}"

def validate_kernel_schema(kernel_data):
    """Validate kernel YAML against expected schema.
    Returns (is_valid, error_message)"""
    schema = {
        "type": "object",
        "required": ["repo", "seed", "sigils", "identity", "actions"],
        "properties": {
            "repo": {"type": "string"},
            "seed": {"type": "string"},
            "sigils": {
                "type": "array",
                "items": {"type": "string"}
            },
            "identity": {"type": "string"},
            "actions": {"type": "string"},
            "prompt": {"type": "string"}
        }
    }
    
    try:
        jsonschema.validate(instance=kernel_data, schema=schema)
        return True, None
    except jsonschema.exceptions.ValidationError as e:
        return False, f"Kernel schema validation error: {str(e)}"

def main():
    parser = argparse.ArgumentParser(description='Process kernel files for Wonder.')
    parser.add_argument('kernel', help='Path to the kernel configuration file')
    parser.add_argument('--output', help='Path for the output file', required=True)
    args = parser.parse_args()
    
    # Get the base directory (where this script is located)
    base_dir = Path(__file__).parent.parent
    
    # Load configuration and schema
    config_path = base_dir / 'control' / 'gizzard-processing.yaml'
    schema_path = base_dir / 'control' / 'gizzard-schema.yaml'
    
    if not config_path.exists():
        print(f"Error: Configuration file not found at {config_path}")
        return
    
    if not schema_path.exists():
        print(f"Error: Schema file not found at {schema_path}")
        return
    
    try:
        processor = GizzardProcessor(str(config_path), str(schema_path))
    except jsonschema.exceptions.ValidationError:
        print("Configuration validation failed. Please check the configuration file.")
        return
    except ValueError as e:
        print(f"Error: {e}")
        print("Please set the WONDER_ROOT environment variable to point to your wonder project root.")
        return
    
    # Load preserve terms if available
    preserve_terms_path = base_dir / 'control' / 'preserve_terms.yaml'
    if preserve_terms_path.exists():
        processor.load_preserve_terms(str(preserve_terms_path))
    
    # Process the kernel
    processor.process_kernel(args.kernel, args.output)

if __name__ == '__main__':
    main() 